#!/bin/bash
#SBATCH --job-name=C6H6_0
#SBATCH --partition=q12,q16,q16l,q20
#SBATCH --mem=30G
#SBATCH --nodes=1
#SBATCH --time=6:00:00
##SBATCH --exclusive
#SBATCH --ntasks-per-node=10
#SBATCH --cpus-per-task=1
#SBATCH --no-requeue
#SBATCH --array=0-2%3


echo "========= Job started  at `date` =========="
# Go to the directory where this job was submitted
cd $SLURM_SUBMIT_DIR

source /home/mkb/.gpaw_py3
source /home/mkb/.GOFEE

# Setup directories to run the search in
base_path=$SLURM_SUBMIT_DIR/runs0/run$SLURM_ARRAY_TASK_ID
mkdir $base_path

# Copy files
cp run_search.py $base_path
cp slab.traj $base_path

# Go to execution directory and run code
cd $base_path
mpiexec --mca mpi_warn_on_fork 0 gpaw-python run_search.py > search.log
echo "========= Job finished at `date` =========="
